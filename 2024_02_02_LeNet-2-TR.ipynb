{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EIz5GR7WkEk6"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CqMbVNNTUIB3"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p6je9gGehvNy"
   },
   "outputs": [],
   "source": [
    "GESTURES = [\"g1\",\"g2\",\"g3\",\"g4\",\"g5\",\"g6\",\"g7\"]\n",
    "SAMPLES_PER_GESTURE = 100\n",
    "NUM_GESTURES = len(GESTURES)\n",
    "ONE_HOT_GESTURES = np.eye(NUM_GESTURES) \n",
    "NUM_SENSOR = 6 \n",
    "\n",
    "inputs = [] \n",
    "outputs = []\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "for g_idx in range(NUM_GESTURES):\n",
    "    g = GESTURES[g_idx]\n",
    "    output = ONE_HOT_GESTURES[g_idx]\n",
    "    df1 = pd.read_csv(\"subject1/\" + g + \".csv\",header = None)\n",
    "    df2 = pd.read_csv(\"subject2/\" + g + \".csv\",header = None)\n",
    "    df3 = pd.read_csv(\"subject3/\" + g + \".csv\",header = None)\n",
    "    df4 = pd.read_csv(\"subject4/\" + g + \".csv\",header = None)\n",
    "    df5 = pd.read_csv(\"subject5/\" + g + \".csv\",header = None)\n",
    "    df6 = pd.read_csv(\"subject6/\" + g + \".csv\",header = None)\n",
    "    df = df5\n",
    "    df_scaled = scaler.fit_transform(df) \n",
    "    df_scaled_DF = pd.DataFrame(df_scaled)  \n",
    "    df_scaled_DF = df_scaled_DF.dropna()   \n",
    "    num_recordings = int(df_scaled_DF.shape[0] / SAMPLES_PER_GESTURE)\n",
    "\n",
    "    for i in range(num_recordings):\n",
    "        sensorData = df_scaled_DF.iloc[i*SAMPLES_PER_GESTURE:(i+1)*SAMPLES_PER_GESTURE,:]\n",
    "        sensorData_np = np.array(sensorData) \n",
    "        sensorData_np = np.reshape(sensorData_np,SAMPLES_PER_GESTURE*NUM_SENSOR)\n",
    "        inputs.append(sensorData_np)\n",
    "        outputs.append(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_np = np.array(inputs)\n",
    "outputs_np = np.array(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_oneshot = np.arange(0,280,40)\n",
    "index_twoshot = np.arange(0,280,20)\n",
    "index_threeshot = np.sort(np.concatenate((index_oneshot+1,index_twoshot)))\n",
    "index_fourshot = np.sort(np.concatenate((index_twoshot+1,index_twoshot)))\n",
    "index_fiveshot = np.sort(np.concatenate((index_oneshot+2,index_fourshot)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_test = np.array([0])\n",
    "for i in range(5,20):\n",
    "    temp  = np.arange(0,280,20)+i\n",
    "    index_test = np.sort(np.concatenate((temp,index_test)))\n",
    "index_test = index_test[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.models.load_model('base_model2.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_rem, X_test, y_rem, y_test = train_test_split(inputs_np,outputs_np, train_size=0.1, stratify = outputs_np)\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(inputs_np[index_twoshot],outputs_np[index_twoshot], train_size=0.5, stratify = outputs_np[index_twoshot])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = inputs_np[index_fourshot]\n",
    "y_train = outputs_np[index_fourshot]\n",
    "\n",
    "X_test = inputs_np[index_test]\n",
    "y_test = outputs_np[index_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = base_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_class = np.argmax(predictions, axis=1)\n",
    "ground_truth = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set()\n",
    "f,ax=plt.subplots()\n",
    "y_true = ground_truth\n",
    "y_pred = predict_class\n",
    "C2= confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(C2,annot=True,ax=ax)\n",
    "ax.set_title('confusion matrix') \n",
    "ax.set_xlabel('predict') \n",
    "ax.set_ylabel('true') \n",
    "plt.savefig('test.png', dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = base_model.evaluate(X_test,y_test,verbose = 0)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinitialize_layer(model, initializer, layer_name):\n",
    "    layer = model.get_layer(layer_name)    \n",
    "    layer.set_weights([initializer(shape=w.shape) for w in layer.get_weights()])\n",
    "    \n",
    "initializer = keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=42)\n",
    "reinitialize_layer(new_model, initializer, \"dense_2\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rSMlPkRw192i",
    "outputId": "efda1599-a867-4b73-c8e1-f71e95ad57a6"
   },
   "outputs": [],
   "source": [
    "# build the model and train it\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "optimizers = [\n",
    "    tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    tf.keras.optimizers.Adam(learning_rate=1e-2)\n",
    "]\n",
    "optimizers_and_layers = [(optimizers[0], new_model.layers[0:-2]), (optimizers[1], new_model.layers[-1])]\n",
    "optimizer = tfa.optimizers.MultiOptimizer(optimizers_and_layers)\n",
    "new_model.compile(optimizer= optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = new_model.fit(X_train, y_train,validation_data=(X_train, y_train), epochs=16, batch_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = new_model.predict(X_test)\n",
    "predict_class2 = np.argmax(predictions2, axis=1)\n",
    "ground_truth = np.argmax(y_test, axis=1)\n",
    "\n",
    "sns.set()\n",
    "f,ax=plt.subplots()\n",
    "y_true2 = ground_truth\n",
    "y_pred2 = predict_class2\n",
    "C22= confusion_matrix(y_true2, y_pred2)\n",
    "sns.heatmap(C22,annot=True,ax=ax) \n",
    "\n",
    "ax.set_title('confusion matrix') \n",
    "ax.set_xlabel('predict')\n",
    "ax.set_ylabel('true') \n",
    "\n",
    "plt.savefig('test2.png', dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = new_model.evaluate(X_test,y_test,verbose = 0)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "NiD8V2zn6tbA",
    "outputId": "1c013cff-78fb-4a2e-a977-d44a1943af0a"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dko9nvxDsRF3"
   },
   "source": [
    "#### "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
